{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_discriminator(x, is_training, num_classes=10, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse) as scope:\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 512, activation=tf.nn.leaky_relu)\n",
    "        x = tf.layers.dense(x, 256, activation=tf.nn.leaky_relu)\n",
    "        features = tf.layers.dense(x, 128, activation=tf.nn.leaky_relu)\n",
    "        logits = tf.layers.dense(features, num_classes + 1)\n",
    "        output = tf.nn.softmax(logits)\n",
    "        return output, logits, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_generator(x, is_training, output_shape=(28, 28, 1), reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse) as scope:\n",
    "        x = tf.layers.dense(x, 256, activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.layers.dense(x, 512, activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.layers.dense(x, np.prod(output_shape), activation=tf.nn.tanh)\n",
    "        x = tf.reshape(x, (-1,) + output_shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_model(x_real, z, is_training, num_classes=10, output_shape=(28, 28, 1)):\n",
    "    d_real_prob, d_real_logits, d_real_features = dense_discriminator(\n",
    "        x_real, is_training, num_classes=num_classes, reuse=False,\n",
    "    )\n",
    "    x_fake = dense_generator(z, is_training, output_shape=output_shape)\n",
    "    d_fake_prob, d_fake_logits, d_fake_features = dense_discriminator(\n",
    "        x_fake, is_training, num_classes=num_classes, reuse=True,\n",
    "    )\n",
    "    return d_real_prob, d_real_logits, d_real_features, d_fake_prob, d_fake_logits, d_fake_features, x_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_loss_accuracy(d_real_prob, d_real_logits, d_real_features,\n",
    "                           d_fake_prob, d_fake_logits, d_fake_features,\n",
    "                           extended_label, labeled_mask):\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    ### Discriminator loss\n",
    "    # Supervised loss for discriminator\n",
    "    d_ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=d_real_logits,\n",
    "                                                      labels=extended_label)\n",
    "    d_loss_supervised = tf.reduce_sum(labeled_mask * d_ce) / (tf.reduce_sum(labeled_mask) + epsilon)\n",
    "    # Unsupervised loss for discriminator\n",
    "    # data is real\n",
    "    # subtract from one due to log --> log of (1 - 0) is 0, therefore loss is 0 for d_real_prob[i, -1] == 0\n",
    "    prob_real_be_real = 1 - d_real_prob[:, -1] + epsilon\n",
    "    logprob = tf.log(prob_real_be_real)\n",
    "    d_loss_unsupervised1 = -1 * tf.reduce_mean(logprob)\n",
    "    # data is fake\n",
    "    prob_fake_be_fake = d_fake_prob[:, -1] + epsilon\n",
    "    logprob = tf.log(prob_fake_be_fake)\n",
    "    d_loss_unsupervised2 = -1 * tf.reduce_mean(logprob)\n",
    "    \n",
    "    d_loss = d_loss_supervised + d_loss_unsupervised1 + d_loss_unsupervised2\n",
    "    \n",
    "    ### Generator loss\n",
    "    # fake data is mistaken to be real\n",
    "    prob_fake_be_real = 1 - d_fake_prob[:, -1] + epsilon\n",
    "    logprob = tf.log(prob_fake_be_real)\n",
    "    g_loss_probs = -1 * tf.reduce_mean(logprob)\n",
    "    \n",
    "    mean_real_features = tf.reduce_mean(d_real_features, axis=0)\n",
    "    mean_fake_features = tf.reduce_mean(d_fake_features, axis=0)\n",
    "    g_loss_fm = tf.reduce_mean(tf.square(mean_real_features - mean_fake_features))\n",
    "    \n",
    "    g_loss = g_loss_probs + g_loss_fm\n",
    "    \n",
    "    ### Accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(d_real_prob[:, :-1], 1),\n",
    "                                  tf.argmax(extended_label[:, :-1], 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return d_loss_supervised, d_loss_unsupervised1, d_loss_unsupervised2, d_loss, g_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(d_loss, g_loss, d_learning_rate, g_learning_rate):\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        all_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in all_vars if var.name.startswith('discriminator')]\n",
    "        g_vars = [var for var in all_vars if var.name.startswith('generator')]\n",
    "\n",
    "        d_optimizer = tf.train.AdamOptimizer(d_learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "        g_optimizer = tf.train.AdamOptimizer(g_learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "        return d_optimizer, g_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_labels(labels):\n",
    "    # add extra label for fake data\n",
    "    extended_label = tf.concat([labels, tf.zeros([tf.shape(labels)[0], 1])], axis=1)\n",
    "\n",
    "    return extended_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, n=10):\n",
    "    ret = np.cumsum(x)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(x_train, y_train, x_test, y_test,\n",
    "            epochs=50000, batch_size=32, test_steps=500, num_labeled_examples=None, \n",
    "            periodic_labeled_batch=False, periodic_labeled_batch_frequency=10,\n",
    "            x_height=28, x_width=28, num_channels=1, latent_size=100):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    num_classes = np.unique(y_train).shape[0]\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, name='x', shape=(None, x_height, x_width, num_channels))\n",
    "    label = tf.placeholder(tf.float32, name='label', shape=(None, num_classes))\n",
    "    labeled_mask = tf.placeholder(tf.float32, name='labeled_mask', shape=(None,))\n",
    "    z = tf.placeholder(tf.float32, name='z', shape=(None, latent_size))\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    g_learning_rate = tf.placeholder(tf.float32, name='g_learning_rate')\n",
    "    d_learning_rate = tf.placeholder(tf.float32, name='d_learning_rate')\n",
    "    \n",
    "    model = build_dense_model(x, z, is_training)\n",
    "    extended_label = extend_labels(label)\n",
    "    d_real_prob, d_real_logits, d_real_features, d_fake_prob, d_fake_logits, d_fake_features, x_fake = model\n",
    "    loss_acc = standard_loss_accuracy(d_real_prob, d_real_logits, d_real_features,\n",
    "                                      d_fake_prob, d_fake_logits, d_fake_features,\n",
    "                                      extended_label, labeled_mask)\n",
    "    _, _, _, d_loss, g_loss, accuracy = loss_acc\n",
    "    d_optimizer, g_optimizer = optimizer(d_loss, g_loss, d_learning_rate, g_learning_rate)\n",
    "    \n",
    "    if num_labeled_examples is None:\n",
    "        global_mask = np.ones(x_train.shape[0])\n",
    "        periodic_labeled_batch = False\n",
    "    else:\n",
    "        global_mask = np.zeros(x_train.shape[0])\n",
    "        for cls in np.unique(y_train):\n",
    "            idx = y_train == cls\n",
    "            idx = np.random.choice(np.flatnonzero(idx), num_labeled_examples // num_classes, replace=False)\n",
    "            global_mask[idx] = 1.0\n",
    "        \n",
    "    \n",
    "    train_d_losses, train_g_losses, train_accuracies = [], [], []\n",
    "    test_d_losses, test_g_losses, test_accuracies = [], [], []\n",
    "    \n",
    "    def test_gan(epoch):\n",
    "        test_size = x_test.shape[0]\n",
    "        z_test = np.random.normal(0, 1, (test_size, latent_size))\n",
    "        test_mask = np.ones(test_size)\n",
    "        test_dictionary = {\n",
    "            x: x_test,\n",
    "            z: z_test,\n",
    "            label: y_test,\n",
    "            labeled_mask: test_mask,\n",
    "            is_training: False\n",
    "        }\n",
    "\n",
    "        test_d_loss = d_loss.eval(feed_dict=test_dictionary)\n",
    "        test_g_loss = g_loss.eval(feed_dict=test_dictionary)\n",
    "        test_accuracy = accuracy.eval(feed_dict=test_dictionary)\n",
    "\n",
    "        test_d_losses.append(test_d_loss)\n",
    "        test_g_losses.append(test_g_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(epoch, test_d_loss, test_g_loss, test_accuracy)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if periodic_labeled_batch and epoch % periodic_labeled_batch_frequency == 0:\n",
    "                idx = np.flatnonzero(global_mask)\n",
    "                x_batch = x_train[idx]\n",
    "                y_batch = y_train[idx]\n",
    "                mask = global_mask[idx]\n",
    "            else:\n",
    "                idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "                x_batch = x_train[idx]\n",
    "                y_batch = y_train[idx]\n",
    "                mask = global_mask[idx]\n",
    "            y_batch = to_categorical(y_batch, num_classes=num_classes)\n",
    "            z_batch = np.random.normal(0, 1, (batch_size, latent_size))\n",
    "            train_dictionary = {\n",
    "                x: x_batch,\n",
    "                z: z_batch,\n",
    "                label: y_batch,\n",
    "                labeled_mask: mask,\n",
    "                g_learning_rate: 0.001,\n",
    "                d_learning_rate: 0.001,\n",
    "                is_training: True\n",
    "            }\n",
    "            d_optimizer.run(feed_dict=train_dictionary)\n",
    "            g_optimizer.run(feed_dict=train_dictionary)\n",
    "\n",
    "            train_d_loss = d_loss.eval(feed_dict=train_dictionary)\n",
    "            train_g_loss = g_loss.eval(feed_dict=train_dictionary)\n",
    "            train_accuracy = accuracy.eval(feed_dict=train_dictionary)\n",
    "            \n",
    "            train_d_losses.append(train_d_loss)\n",
    "            train_g_losses.append(train_g_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            if epoch % test_steps == 0:\n",
    "                test_gan(epoch)\n",
    "        test_gan(epochs)\n",
    "    \n",
    "    return train_d_losses, train_g_losses, train_accuracies, test_d_losses, test_g_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(output_file, num_labeled_examples=None,\n",
    "             periodic_labeled_batch=False, periodic_labeled_batch_frequency=10):\n",
    "    results = execute(x_train, y_train, x_test, y_test)\n",
    "    train_d_losses, train_g_losses, train_accuracies, test_d_losses, test_g_losses, test_accuracies = results\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'train_d_losses': train_d_losses,\n",
    "            'train_g_losses': train_g_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_d_losses': test_d_losses,\n",
    "            'test_g_losses': test_g_losses,\n",
    "            'test_accuracies': test_accuracies\n",
    "        }, f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(results):\n",
    "    train_d_losses, train_g_losses, _, test_d_losses, test_g_losses, _ = results\n",
    "    \n",
    "    average_train_d_losses = moving_average(train_d_losses, 10)\n",
    "    average_train_g_losses = moving_average(train_g_losses, 10)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(np.arange(len(average_train_d_losses)), average_train_d_losses, label='discriminator training loss')\n",
    "    plt.plot(np.arange(len(average_train_g_losses)), average_train_g_losses, label='generator training loss')\n",
    "    plt.plot(np.arange(len(test_d_losses)) * 500, test_d_losses, label='discriminator test loss')\n",
    "    plt.plot(np.arange(len(test_g_losses)) * 500, test_g_losses, label='generator test loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(results):\n",
    "    _, _, train_accuracies, _, _, test_accuracies = results\n",
    "    \n",
    "    average_train_accuracies = moving_average(train_accuracies, 10)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(np.arange(len(average_train_accuracies)), average_train_accuracies, label='training accuracy')\n",
    "    plt.plot(np.arange(len(test_accuracies)) * 500, test_accuracies, label='test accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test('improved-gan-all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
